name: Daily Backup

on:
  workflow_run:
    workflows: ["Update Stock Data"]
    types:
      - completed
  workflow_dispatch:  # Manual trigger

jobs:
  backup:
    runs-on: self-hosted
    # Run only if update-data succeeded or manually triggered
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    env:
      HAS_RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG != '' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install supabase pandas python-dotenv

      - name: Export Supabase data
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python -c "
          import os
          import pandas as pd
          from datetime import date
          from supabase import create_client

          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_KEY']
          client = create_client(url, key)

          today = date.today().strftime('%Y%m%d')
          backup_dir = 'data/backup'
          os.makedirs(backup_dir, exist_ok=True)

          # Export companies
          result = client.table('companies').select('*').execute()
          if result.data:
              df = pd.DataFrame(result.data)
              df.to_csv(f'{backup_dir}/companies_{today}.csv', index=False)
              print(f'Exported {len(result.data)} companies')

          # Export metrics (latest only)
          result = client.table('metrics').select('*').execute()
          if result.data:
              df = pd.DataFrame(result.data)
              df.to_csv(f'{backup_dir}/metrics_{today}.csv', index=False)
              print(f'Exported {len(result.data)} metrics')
          "

      - name: Cleanup old prices (keep 1 month)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python -c "
          import os
          from datetime import datetime, timedelta
          from supabase import create_client

          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_KEY']
          client = create_client(url, key)

          # Delete prices older than 1 month
          one_month_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
          result = client.table('prices').delete().lt('date', one_month_ago).execute()
          deleted_count = len(result.data) if result.data else 0
          print(f'Deleted {deleted_count} old price records (before {one_month_ago})')
          "

      - name: Upload backup artifact (GitHub - 30 days retention)
        uses: actions/upload-artifact@v4
        with:
          name: supabase-backup-${{ github.run_id }}
          path: data/backup/*.csv
          retention-days: 30

      # ============================================================
      # Google Drive Backup (Permanent Storage)
      # ============================================================
      # Requires RCLONE_CONFIG secret to be set in GitHub repository settings.
      # See .claude/rules/data-policy.md for setup instructions.
      # ============================================================

      - name: Setup rclone
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        env:
          RCLONE_CONFIG_CONTENT: ${{ secrets.RCLONE_CONFIG }}
        run: |
          # Skip install if already installed (self-hosted runner)
          if ! command -v rclone &> /dev/null; then
            curl -s https://rclone.org/install.sh | sudo bash
          fi
          # Use existing config on self-hosted runner, or create from secret
          if [ ! -f ~/.config/rclone/rclone.conf ]; then
            mkdir -p ~/.config/rclone
            echo "$RCLONE_CONFIG_CONTENT" > ~/.config/rclone/rclone.conf
          fi

      - name: Upload to Google Drive
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        run: |
          # Upload Supabase backup CSVs only
          # (prices/financials are backed up by update-data.yml)
          echo "Uploading Supabase backup CSVs to Google Drive..."
          rclone copy data/backup/ gdrive:backups/ --progress
          echo "Google Drive backup complete!"

      - name: Cleanup old backups (keep 4 weeks rolling)
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        run: |
          # Delete backups older than 28 days from Google Drive
          rclone delete gdrive:backups/ --min-age 28d --progress 2>/dev/null || true
