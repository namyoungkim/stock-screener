name: Daily Backup

on:
  workflow_run:
    workflows: ["Update Stock Data"]
    types:
      - completed
  workflow_dispatch:  # Manual trigger

jobs:
  backup:
    runs-on: self-hosted
    # Run only if update-data succeeded or manually triggered
    if: ${{ github.event_name == 'workflow_dispatch' || github.event.workflow_run.conclusion == 'success' }}
    env:
      HAS_RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG != '' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install supabase pandas python-dotenv

      - name: Export Supabase data
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python -c "
          import os
          import pandas as pd
          from datetime import date
          from supabase import create_client

          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_KEY']
          client = create_client(url, key)

          # New structure: data/supabase/YYYY-MM-DD/
          today = date.today().strftime('%Y-%m-%d')
          backup_dir = f'data/supabase/{today}'
          os.makedirs(backup_dir, exist_ok=True)

          def export_table(client, table_name, output_path):
              '''Export all rows from a table with pagination.'''
              all_data = []
              offset = 0
              page_size = 1000

              while True:
                  result = client.table(table_name).select('*').range(offset, offset + page_size - 1).execute()
                  if not result.data:
                      break
                  all_data.extend(result.data)
                  if len(result.data) < page_size:
                      break
                  offset += page_size

              if all_data:
                  df = pd.DataFrame(all_data)
                  df.to_csv(output_path, index=False)
                  print(f'Exported {len(all_data):,} {table_name}')

          export_table(client, 'companies', f'{backup_dir}/companies.csv')
          export_table(client, 'metrics', f'{backup_dir}/metrics.csv')
          export_table(client, 'prices', f'{backup_dir}/prices.csv')
          "

      - name: Cleanup old prices (keep 1 month)
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python -c "
          import os
          from datetime import datetime, timedelta
          from supabase import create_client

          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_KEY']
          client = create_client(url, key)

          # Delete prices older than 1 month
          one_month_ago = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-%d')
          result = client.table('prices').delete().lt('date', one_month_ago).execute()
          deleted_count = len(result.data) if result.data else 0
          print(f'Deleted {deleted_count} old price records (before {one_month_ago})')
          "

      - name: Upload backup artifact (GitHub - 30 days retention)
        uses: actions/upload-artifact@v4
        with:
          name: supabase-backup-${{ github.run_id }}
          path: data/supabase/
          retention-days: 30

      # ============================================================
      # Google Drive Backup (Permanent Storage)
      # ============================================================
      # Requires RCLONE_CONFIG secret to be set in GitHub repository settings.
      # See .claude/rules/data-policy.md for setup instructions.
      # ============================================================

      - name: Setup rclone
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        env:
          RCLONE_CONFIG_CONTENT: ${{ secrets.RCLONE_CONFIG }}
        run: |
          # Skip install if already installed (self-hosted runner)
          if ! command -v rclone &> /dev/null; then
            curl -s https://rclone.org/install.sh | sudo bash
          fi
          # Use existing config on self-hosted runner, or create from secret
          if [ ! -f ~/.config/rclone/rclone.conf ]; then
            mkdir -p ~/.config/rclone
            echo "$RCLONE_CONFIG_CONTENT" > ~/.config/rclone/rclone.conf
          fi

      - name: Upload to Google Drive
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        run: |
          # Upload Supabase backup to gdrive:supabase/YYYY-MM-DD/
          echo "Uploading Supabase backup CSVs to Google Drive..."
          rclone copy data/supabase/ gdrive:supabase/ --progress
          echo "Google Drive backup complete!"

      - name: Cleanup old backups (keep 4 weeks rolling)
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        run: |
          # Delete backups older than 28 days from Google Drive
          rclone delete gdrive:supabase/ --min-age 28d --progress 2>/dev/null || true
