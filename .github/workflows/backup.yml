name: Daily Backup

on:
  schedule:
    # Every weekday at 00:00 UTC (after data collection)
    - cron: '0 1 * * 1-5'
  workflow_dispatch:  # Manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    env:
      HAS_RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG != '' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install supabase pandas python-dotenv

      - name: Export Supabase data
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        run: |
          python -c "
          import os
          import pandas as pd
          from datetime import date
          from supabase import create_client

          url = os.environ['SUPABASE_URL']
          key = os.environ['SUPABASE_KEY']
          client = create_client(url, key)

          today = date.today().strftime('%Y%m%d')
          backup_dir = 'data/backup'
          os.makedirs(backup_dir, exist_ok=True)

          # Export companies
          result = client.table('companies').select('*').execute()
          if result.data:
              df = pd.DataFrame(result.data)
              df.to_csv(f'{backup_dir}/companies_{today}.csv', index=False)
              print(f'Exported {len(result.data)} companies')

          # Export metrics (latest only)
          result = client.table('metrics').select('*').execute()
          if result.data:
              df = pd.DataFrame(result.data)
              df.to_csv(f'{backup_dir}/metrics_{today}.csv', index=False)
              print(f'Exported {len(result.data)} metrics')
          "

      - name: Create backup archive
        run: |
          cd data/backup
          DATE=$(date +%Y%m%d)
          tar -czvf backup_${DATE}.tar.gz *.csv 2>/dev/null || echo "No CSV files to archive"

      - name: Upload backup artifact (GitHub - 30 days retention)
        uses: actions/upload-artifact@v4
        with:
          name: supabase-backup-${{ github.run_id }}
          path: data/backup/*.tar.gz
          retention-days: 30

      # ============================================================
      # Google Drive Backup (Permanent Storage)
      # ============================================================
      # Requires RCLONE_CONFIG secret to be set in GitHub repository settings.
      # See .claude/rules/data-policy.md for setup instructions.
      # ============================================================

      - name: Setup rclone
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        env:
          RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}
        run: |
          curl -s https://rclone.org/install.sh | sudo bash
          mkdir -p ~/.config/rclone
          echo "$RCLONE_CONFIG" > ~/.config/rclone/rclone.conf

      - name: Upload to Google Drive
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        env:
          RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}
        run: |
          DATE=$(date +%Y%m%d)

          # Upload Supabase backup
          echo "Uploading Supabase backup to Google Drive..."
          rclone copy data/backup/backup_${DATE}.tar.gz gdrive:stock-screener-backup/backups/ --progress

          # Upload daily data files if they exist
          if [ -d "data/prices" ]; then
            echo "Uploading price data..."
            rclone copy data/prices/ gdrive:stock-screener-backup/prices/ --progress
          fi

          if [ -d "data/financials" ]; then
            echo "Uploading financial data..."
            rclone copy data/financials/ gdrive:stock-screener-backup/financials/ --progress
          fi

          echo "Google Drive backup complete!"

      - name: Cleanup old backups (keep 4 weeks rolling)
        if: ${{ env.HAS_RCLONE_CONFIG == 'true' }}
        env:
          RCLONE_CONFIG: ${{ secrets.RCLONE_CONFIG }}
        run: |
          # List and delete backups older than 28 days from Google Drive
          rclone delete gdrive:stock-screener-backup/backups/ --min-age 28d --progress 2>/dev/null || true

      - name: Local cleanup
        run: |
          # Keep only the latest backup archive locally
          cd data/backup
          ls -t *.tar.gz 2>/dev/null | tail -n +2 | xargs -r rm
          rm -f *.csv 2>/dev/null || true
